import os
import torch
from transformers import GPT2LMHeadModel, GPT2TokenizerFast

#  1. í›ˆë ¨ëœ KoGPT2 ëª¨ë¸ ë¡œë“œ
model_path = os.path.abspath("./trained_kogpt2")  # ì ˆëŒ€ ê²½ë¡œ ì„¤ì •
tokenizer = GPT2TokenizerFast.from_pretrained(model_path)
model = GPT2LMHeadModel.from_pretrained(model_path)

#  2. ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ë³€ê²½
model.eval()

#  3. ë‰´ìŠ¤, ì—°ì„¤ë¬¸ ë“±ì˜ ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì°¨ë‹¨
bad_words = ["ëŒ€í†µë ¹", "ê¸°ìƒì²­", "ì—°ì„¤", "ê³µì‹", "ê¸°ì", "ê¸°ë…ì‹", "ë³´ë„", "ì¤‘êµ­", "ì¼ë³¸", "ìŠ¤ìŠ¹ì˜ ë‚ "]
bad_words_ids = [tokenizer.encode(word, add_special_tokens=False) for word in bad_words]

#  4. ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ì±—ë´‡ì´ ëŒ€ë‹µí•˜ë„ë¡ ì„¤ì •
def chat_with_bot():
    print("\nğŸ”¹ KoGPT2 ì±—ë´‡ í…ŒìŠ¤íŠ¸ ì‹œì‘! (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥)\n")

    while True:
        user_input = input("ğŸ˜ƒ ë‹¹ì‹ : ").strip()
        if user_input.lower() in ["exit", "ì¢…ë£Œ", "quit"]:
            print("ğŸ’¬ ì±—ë´‡: ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”!")
            break

        #  5. ì§ˆë¬¸ ì…ë ¥ì„ KoGPT2ê°€ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë³€í™˜
        input_text = f"ì§ˆë¬¸: {user_input}\nëŒ€ë‹µ:"
        input_ids = tokenizer.encode(input_text, return_tensors="pt")

        #  6. attention_mask ì¶”ê°€í•˜ì—¬ `generate()` ë¬¸ì œ í•´ê²°
        attention_mask = input_ids.ne(tokenizer.pad_token_id).long()  # pad_token_idê°€ ì•„ë‹Œ ë¶€ë¶„ë§Œ 1ë¡œ ì„¤ì •

        #  7. KoGPT2ê°€ ë‹µë³€ ìƒì„±
        with torch.no_grad():
            output = model.generate(
                input_ids,
                attention_mask=attention_mask,  # ğŸ”¹ attention_mask ì¶”ê°€
                max_length=50,  # ğŸ”¹ ë„ˆë¬´ ì§§ê²Œ ì˜ë¦¬ëŠ” ë¬¸ì œ í•´ê²° (ê¸°ì¡´ 30 â†’ 50)
                num_return_sequences=1,  # ìƒì„±í•  ì‘ë‹µ ê°œìˆ˜
                top_p=0.85,  # ğŸ”¹ nucleus sampling ë²”ìœ„ ì¡°ì • (ê¸°ì¡´ 0.8 â†’ 0.85)
                temperature=0.7,  # ğŸ”¹ ì¡°ê¸ˆ ë” ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ ìƒì„± (ê¸°ì¡´ 0.6 â†’ 0.7)
                repetition_penalty=1.8,  # ğŸ”¹ ë°˜ë³µë˜ëŠ” ë¬¸ì¥ ì–µì œ (ê¸°ì¡´ 1.7 â†’ 1.8)
                no_repeat_ngram_size=2,  # ğŸ”¹ 2ê°œ ë‹¨ì–´ ì´ìƒì˜ n-ê·¸ë¨ ë°˜ë³µ ë°©ì§€
                do_sample=True,  # ìƒ˜í”Œë§ ì ìš©
                bad_words_ids=bad_words_ids,  # ğŸ”¹ ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì°¨ë‹¨
                eos_token_id=tokenizer.eos_token_id  # ğŸ”¹ ë¬¸ì¥ ì™„ì„±ì„ ìœ ë„
            )

        #  8. KoGPT2ì˜ ëŒ€ë‹µì„ ë””ì½”ë”©í•˜ì—¬ ì¶œë ¥
        response = tokenizer.decode(output[0], skip_special_tokens=True)
        response = response.replace(input_text, "").strip()  # ì…ë ¥ ì§ˆë¬¸ ì œê±°

        #  9. íŠ¹ìˆ˜ ë¬¸ì í•„í„°ë§ ë° ì‘ë‹µ ì •ì œ
        response = response.split("ëŒ€ë‹µ:")[0].strip()  # ë¶ˆí•„ìš”í•œ ë°˜ë³µ ì œê±°
        response = response.replace("í•˜ì§€ë§Œ í•˜ì§€ë§Œ", "í•˜ì§€ë§Œ")  # ë°˜ë³µëœ ì—°ê²°ì–´ ì œê±°

        print(f"ğŸ’¬ ì±—ë´‡: {response}\n")


chat_with_bot()
